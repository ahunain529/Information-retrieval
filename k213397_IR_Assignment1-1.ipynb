{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docIDs():\n",
    "    curr_dir = os.getcwd() # get the current directory\n",
    "    docID = [] # create a list to store the document IDs\n",
    "    for i in os.listdir(curr_dir + '\\ResearchPapers'): # loop through each file in the 'ResearchPapers' directory\n",
    "        i = i.rstrip('.txt')\n",
    "        docID.append(int(i))\n",
    "    docID.sort()\n",
    "    return docID\n",
    "\n",
    "def get_stopwords():\n",
    "    stopwords = []\n",
    "    f = open('Stopword-List.txt', 'r') # open the 'Stopword-List.txt' file\n",
    "    while True:\n",
    "        line = f.readline() # each line from the file is read one by one\n",
    "        if not line: # if the line read is empty (which means end of file), the loop is broken\n",
    "            break\n",
    "        stopwords.append(line) # else append the read line to the stopwords list\n",
    "\n",
    "    f.close() # close the file\n",
    "\n",
    "    for i in range(len(stopwords)):\n",
    "        if i != '\\n' and i != '':\n",
    "            stopwords[i] = stopwords[i].rstrip(' \\n') # remove newline characters from the strings\n",
    "        else:\n",
    "            stopwords.pop(i) # remove any empty strings and newline characters from the stopwords list\n",
    "    return stopwords\n",
    "\n",
    "def create_pos_index(total_tokens):\n",
    "    pos_ind = {} # declare an empty dictionary for the positional index\n",
    "    porter_stemmer = PorterStemmer() # initialize the stemmer\n",
    "\n",
    "    # get the stopwords. Although stopwords is not going to be inserted in the positional index, we still need them to find the correct positions of the rest of the words\n",
    "    stopwords = get_stopwords()\n",
    "    doc = get_docIDs() # get the docIDs\n",
    "\n",
    "    for i, tokens in enumerate(total_tokens): # loop through each token in total_tokens, and then loop through each word in the token\n",
    "        for j, word in enumerate(tokens):\n",
    "            if word not in stopwords: # filter the stopwords\n",
    "                word = porter_stemmer.stem(word) # Stem the word\n",
    "                if word[-1] == \"'\": #if the word ends with an apostrophe, remove it\n",
    "                    word = word.rstrip(\"'\")\n",
    "                if word in pos_ind: # if the word is already in the positional index, add the docID to the index\n",
    "                    if doc[i] in pos_ind[word]: # if the docID is already in the index for that word, add the position\n",
    "                        pos_ind[word][doc[i]].append(j)\n",
    "                    else: # else add the docID as well as the position\n",
    "                        pos_ind[word][doc[i]] = [j]\n",
    "                else: # add the word in the index along with the docID and the position\n",
    "                    pos_ind[word] = {doc[i]: [j]}\n",
    "    return pos_ind\n",
    "\n",
    "def create_inv_index(total_tokens):\n",
    "    inv_ind = {} # an empty dictionary for the inverted index\n",
    "    porter_stemmer = PorterStemmer() # initialize the stemmer\n",
    "\n",
    "    # get the stopwords. Although stopwords is not going to be inserted in the positional index, we still need them to find the correct positions of the rest of the words\n",
    "    stopwords = get_stopwords()\n",
    "    doc = get_docIDs() # get the docIDs\n",
    "\n",
    "    for i, tokens in enumerate(total_tokens): # loop through each token in total_tokens again\n",
    "        for word in tokens: # loop through each word in tokens\n",
    "            word = porter_stemmer.stem(word) # stem the word\n",
    "            if word[-1] == \"'\": # remove the apostrophe\n",
    "                word = word.rstrip(\"'\")\n",
    "            if word in inv_ind: # if the word is already in the inverted index\n",
    "                if doc[i] not in inv_ind[word]: # append the docID if it isn't in the index\n",
    "                    inv_ind[word].append(doc[i])\n",
    "            else: # add the word along with the docID\n",
    "                inv_ind[word] = [doc[i]]\n",
    "    return inv_ind \n",
    "\n",
    "def tokenization():\n",
    "    total_tokens = [] # an empty list to store the tokens from all the files\n",
    "    doc = get_docIDs() # get the docIDs\n",
    "\n",
    "    for i in doc: # iterate through each doc\n",
    "        tokens = []\n",
    "        f = open('ResearchPapers/' + str(i) +'.txt', 'r') # open the file according to the current document ID\n",
    "        while True:\n",
    "            line = f.readline() # read a line from the file\n",
    "            if not line: # if the line is empty (which means end of file), break the loop\n",
    "                break\n",
    "            tokens += word_tokenize(line) # tokenize the line and add the tokens to the list\n",
    "        f.close() # close the file\n",
    "\n",
    "        j = 0\n",
    "        while j < len(tokens): # loop through each token\n",
    "            # remove symbols and numbers from the start and end of the token and convert it to lowercase (case folding)\n",
    "            tokens[j] = tokens[j].lstrip('0123456789!@#$%^&*()-_=+[{]}\\|;:\\'\",<.>/?`~')\n",
    "            tokens[j] = tokens[j].rstrip('0123456789!@#$%^&*()-_=+[{]}\\|;:\\'\",<.>/?`~')\n",
    "            tokens[j] = tokens[j].lower()\n",
    "            if '.' in tokens[j]: # if '.' exists in a word, split the word at that point and add the splitted words at the end of the tokens list while removing the original word\n",
    "                word = tokens[j].split('.')\n",
    "                del tokens[j]\n",
    "                tokens.extend(word)\n",
    "            elif '-' in tokens[j]: # do the same for words with '-'\n",
    "                word = tokens[j].split('-')\n",
    "                del tokens[j]\n",
    "                tokens.extend(word)\n",
    "            j += 1 # move the index forward\n",
    "\n",
    "        tokens = [c for c in tokens if c.isalpha()] # filter out any strings that contain symbols, numbers, etc.\n",
    "        total_tokens.append(tokens)\n",
    "    return total_tokens\n",
    "\n",
    "def build_index():\n",
    "    tokens = tokenization() # preprocessing function is called\n",
    "    inv_ind = create_inv_index(tokens) # create_inv_index function is called\n",
    "    pos_ind = create_pos_index(tokens) # create_pos_index function is called\n",
    "\n",
    "    f = open('inv_index.txt', 'w') # the inverted index is written to 'inv_index.txt'\n",
    "    for key, value in inv_ind.items(): # loop through each key-value pair in the inverted index and output it to the file\n",
    "        f.write('{}:'.format(key))\n",
    "        for i in value:\n",
    "            f.write('{} '.format(i))\n",
    "        f.write('\\n')\n",
    "    f.close() # close the file\n",
    "\n",
    "    f = open('pos_index.txt', 'w') # do the same for positional index\n",
    "    for key, value in pos_ind.items():\n",
    "        for k, v in value.items():\n",
    "            f.write('{}:{}.'.format(key, k))\n",
    "            for i in v:\n",
    "                f.write('{} '.format(i))\n",
    "            f.write('\\n')\n",
    "    f.close() # close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_index() # execute the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_inv_index():\n",
    "    inv_index = {}\n",
    "    \n",
    "    f = open('inv_index.txt', 'r') # open the inverted index file in read mode\n",
    "    while True:\n",
    "        line = f.readline() # read a line from the file\n",
    "        if line == '': # if the line is empty (which means end of file), break the loop\n",
    "            break\n",
    "        line = line.split(':') # split the line at ':'\n",
    "        term = line[0]\n",
    "        pos = line[1].split()\n",
    "        pos = [int(c) for c in pos if c != '' and c != '\\n' and not c.isalpha()]\n",
    "        inv_index[term] = pos # add the term and its posting list to the inverted index\n",
    "    f.close() # close the file\n",
    "\n",
    "    return inv_index\n",
    "\n",
    "def extract_pos_index():\n",
    "    pos_index = {}\n",
    "\n",
    "    f = open('pos_index.txt', 'r') # do the same for the positional index\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        line = line.split(':')\n",
    "        term = line[0]\n",
    "        docID, pos = line[1].split('.') # this time the second part of the split line is the document ID and the positions\n",
    "        pos = pos.split() # split the positions at whitespace\n",
    "        pos = [int(c) for c in pos if c != '' and c != '\\n' and not c.isalpha()]\n",
    "        if term in pos_index: # if the term is already in the positional index, append the positions to the term's dictionary. Otherwise append both docID and the positions\n",
    "            pos_index[term][int(docID)] = pos\n",
    "        else:\n",
    "            pos_index[term] = {int(docID): pos}\n",
    "    f.close() # close the file\n",
    "\n",
    "    return pos_index\n",
    "\n",
    "def find_AND(p1, p2):\n",
    "    result = []\n",
    "    for item in p1:\n",
    "        if item in p2 and item not in result: # check if the element is also present in p2 and not already in the intersection list\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "def find_OR(p1, p2):\n",
    "    result = []\n",
    "    # add the elements of p1 to the new list while avoiding duplicates\n",
    "    for i in p1:\n",
    "        if i not in result:\n",
    "            result.append(i)\n",
    "\n",
    "    # add elements from p2 to the new list while avoiding duplicates\n",
    "    for i in p2:\n",
    "        if i not in result:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "def find_NOT(p1):\n",
    "    result = []\n",
    "    doc = get_docIDs() # get the list of all document IDs\n",
    "    # find elements in p1 that are not in doc\n",
    "    for i in p1:\n",
    "        if i not in doc and i not in result:\n",
    "            result.append(i)\n",
    "\n",
    "    # find elements in doc that are not in p1\n",
    "    for i in doc:\n",
    "        if i not in p1 and i not in result:\n",
    "            result.append(i)\n",
    "\n",
    "def boolean_query(query):\n",
    "    query = query.split() # split the query into words\n",
    "    porter_stemmer = PorterStemmer() # initialize the stemmer\n",
    "    stopwords = get_stopwords() # get the stopwords\n",
    "    inv_index = extract_inv_index() # Extract the inverted index and positional index\n",
    "\n",
    "    for i, word in enumerate(query): # Loop through each word in the query\n",
    "        if word in ['AND', 'OR', 'NOT']: # If the word is a boolean operator, skip it\n",
    "            continue\n",
    "        temp = porter_stemmer.stem(word) # Stem the word\n",
    "        if temp[-1] == \"'\": # Remove the apostrophe\n",
    "            temp = word.rstrip(\"'\")\n",
    "        if word in stopwords: # Remove the stopwords\n",
    "            query.remove(word)\n",
    "        else:\n",
    "            query[i] = temp\n",
    "\n",
    "    if 'AND' in query: # if the query contains 'AND', 'OR' or 'NOT, split the query at that point and process the two parts separately\n",
    "        index = query.index('AND')\n",
    "        t1 = query[:index] # splitting the query into two parts\n",
    "        t2 = query[index+1:]\n",
    "\n",
    "        # combine the query into a string and recursively call the function to process the first part\n",
    "        temp = ''\n",
    "        for i in t1:\n",
    "            temp += i + ' '\n",
    "        t1 = temp.rstrip(' ')\n",
    "\n",
    "        p1 = boolean_query(t1)\n",
    "        p1 = p1.split() # split the result into a list\n",
    "        temp = []\n",
    "        for i in range(len(p1)): # convert the elements of the list to integers\n",
    "            temp.append(int(p1[i]))\n",
    "        p1 = temp\n",
    "\n",
    "        # combine the query into a string and recursively call the function to process the second part\n",
    "        temp = ''\n",
    "        for i in t2:\n",
    "            temp += i + ' '\n",
    "        t2 = temp.rstrip(' ')\n",
    "    \n",
    "        p2 = boolean_query(t2)\n",
    "        p2 = p2.split()\n",
    "        temp = []\n",
    "        for i in range(len(p2)): # convert the elements of the list to integers\n",
    "            temp.append(int(p2[i]))\n",
    "        p2 = temp\n",
    "\n",
    "        result = find_AND(p1, p2) # find the intersection of the results\n",
    "    elif 'OR' in query:\n",
    "        index = query.index('OR')\n",
    "        t1 = query[:index]\n",
    "        t2 = query[index+1:]\n",
    "        # combine the query into a string and recursively call the function to process the first part\n",
    "        temp = ''\n",
    "        for i in t1:\n",
    "            temp += i + ' ' # convert the list into a string, with each term separated by a space\n",
    "        t1 = temp.rstrip(' ')\n",
    "\n",
    "        p1 = boolean_query(t1)\n",
    "        p1 = p1.split() # split the result into a list\n",
    "        temp = []\n",
    "        for i in range(len(p1)): # convert the elements of the list to integers\n",
    "            temp.append(int(p1[i]))\n",
    "        p1 = temp\n",
    "\n",
    "        # combine the query into a string and recursively call the function to process the second part\n",
    "        temp = ''\n",
    "        for i in t2: # convert the list into a string, with each term separated by a space\n",
    "            temp += i + ' '\n",
    "        t2 = temp.rstrip(' ')\n",
    "    \n",
    "        p2 = boolean_query(t2)\n",
    "        p2 = p2.split()\n",
    "        temp = []\n",
    "        for i in range(len(p2)): # convert the elements of the list to integers\n",
    "            temp.append(int(p2[i]))\n",
    "        p2 = temp\n",
    "\n",
    "        result = find_OR(p1, p2) # find the union of the results\n",
    "    elif 'NOT' in query:\n",
    "        index = query.index('NOT')\n",
    "        t1 = query[index+1:]\n",
    "        # combine the query into a string and recursively call the function to process the first part\n",
    "        temp = ''\n",
    "        for i in t1: # convert the list into a string, with each term separated by a space\n",
    "            temp += i + ' '\n",
    "        t1 = temp.rstrip(' ')\n",
    "\n",
    "        p1 = boolean_query(t1)\n",
    "        p1 = p1.split() # split the result into a list\n",
    "        temp = []\n",
    "        for i in range(len(p1)): # convert the elements of the list to integers\n",
    "            temp.append(int(p1[i]))\n",
    "        p1 = temp\n",
    "\n",
    "        result = find_NOT(p1)\n",
    "    else: # if the query contains only a single term\n",
    "        term = query[0] # extract the term\n",
    "        result = inv_index.get(term, []) # get the postings list for the term from the inverted index\n",
    "\n",
    "    result = ''.join([str(c) + ' ' for c in result]) # convert the result to a string\n",
    "    return result\n",
    "\n",
    "def proximity_query(query):\n",
    "    query = query.split() # split the query into words\n",
    "    porter_stemmer = PorterStemmer() # initialize the stemmer\n",
    "    stopwords = get_stopwords() # get the stopwords\n",
    "    inv_index = extract_inv_index() # extract the inverted and positional indexes\n",
    "    pos_index = extract_pos_index()\n",
    "    pos = query.pop(-1)\n",
    "    pos = int(pos[-1])\n",
    "\n",
    "    query = [c for c in query if c not in stopwords] # remove the stopwords from the query and also apply case folding\n",
    "\n",
    "    word = porter_stemmer.stem(query[0].lower()) # stem the first word in the query\n",
    "    if word[-1] == \"'\": # remove the apostrophe\n",
    "            word = word.rstrip(\"'\")\n",
    "    query[0] = word\n",
    "    word = porter_stemmer.stem(query[1].lower()) # stem the second word in the query\n",
    "    if word[-1] == \"'\": # remove the apostrophe\n",
    "        word = word.rstrip(\"'\")\n",
    "    query[1] = word\n",
    "\n",
    "    docs = [] # create a list to store the postings list for each term in the query\n",
    "    docs.append(inv_index[query[0]]) # get the postings list for the first term\n",
    "    docs.append(inv_index[query[1]]) # get the postings list for the second term\n",
    "\n",
    "    common_docs = find_AND(docs[0], docs[1]) # find the common documents in the postings list of the two terms\n",
    "\n",
    "    # find the postional intersection of the two terms in the common documents\n",
    "    result = [] # create a list to store the result\n",
    "    for i in common_docs: # loop through the common documents\n",
    "        p1 = pos_index.get([query[0]])[i] # get the positions of the first term in the document\n",
    "        p2 = pos_index.get([query[1]])[i] # get the positions of the second term in the document\n",
    "\n",
    "        # now we need to find the positions of the second term that are within the specified proximity of the positions of the first term\n",
    "        j = k = 0\n",
    "        while j != len(p1):\n",
    "            while k != len(p2):\n",
    "                if abs(p1[j] - p2[k]) <= pos: # if the positions of the two terms are within the specified proximity, add the document to the result\n",
    "                    if i not in result: # if the document is not already in the result, add it\n",
    "                        result.append(i)\n",
    "                elif p2[k] > p1[j]: # if the position of the second term is greater than the position of the first term, break the loop\n",
    "                    break \n",
    "                k+=1\n",
    "            j+=1\n",
    "\n",
    "    temp = result\n",
    "    result = ''\n",
    "    for i in range(len(temp)): # convert the result to a string\n",
    "        result.append(str(temp[i]) + ' ')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\tkinter\\__init__.py\", line 1967, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\2682682125.py\", line 7, in search\n",
      "    result = boolean_query(query)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\4245261985.py\", line 161, in boolean_query\n",
      "    p1 = boolean_query(t1)\n",
      "         ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\4245261985.py\", line 173, in boolean_query\n",
      "    result = ''.join([str(c) + ' ' for c in result]) # convert the result to a string\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\tkinter\\__init__.py\", line 1967, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\2682682125.py\", line 5, in search\n",
      "    result = proximity_query(query)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\4245261985.py\", line 205, in proximity_query\n",
      "    p1 = pos_index.get([query[0]])[i] # get the positions of the first term in the document\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: unhashable type: 'list'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\tkinter\\__init__.py\", line 1967, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\2682682125.py\", line 5, in search\n",
      "    result = proximity_query(query)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\4245261985.py\", line 183, in proximity_query\n",
      "    pos = int(pos[-1])\n",
      "          ^^^^^^^^^^^^\n",
      "ValueError: invalid literal for int() with base 10: 'g'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\tkinter\\__init__.py\", line 1967, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\2682682125.py\", line 5, in search\n",
      "    result = proximity_query(query)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\4245261985.py\", line 205, in proximity_query\n",
      "    p1 = pos_index.get([query[0]])[i] # get the positions of the first term in the document\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: unhashable type: 'list'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\tkinter\\__init__.py\", line 1967, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\2682682125.py\", line 5, in search\n",
      "    result = proximity_query(query)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\4245261985.py\", line 205, in proximity_query\n",
      "    p1 = pos_index.get([query[0]])[i] # get the positions of the first term in the document\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: unhashable type: 'list'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\tkinter\\__init__.py\", line 1967, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\2682682125.py\", line 5, in search\n",
      "    result = proximity_query(query)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\4245261985.py\", line 205, in proximity_query\n",
      "    p1 = pos_index.get([query[0]])[i] # get the positions of the first term in the document\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: unhashable type: 'list'\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\tkinter\\__init__.py\", line 1967, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\2682682125.py\", line 5, in search\n",
      "    result = proximity_query(query)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ahuna\\AppData\\Local\\Temp\\ipykernel_9476\\4245261985.py\", line 205, in proximity_query\n",
      "    p1 = pos_index.get([query[0]])[i] # get the positions of the first term in the document\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: unhashable type: 'list'\n"
     ]
    }
   ],
   "source": [
    "def search():\n",
    "    query = query_entry.get()  # get the query from the entry widget\n",
    "\n",
    "    if '/' in query: # if the query contains a '/', it is a proximity query, so call the ProxQueryProcessing function\n",
    "        result = proximity_query(query)\n",
    "    else:  # otherwise, it is a boolean query, so call the BoolQueryProcessing function\n",
    "        result = boolean_query(query)\n",
    "\n",
    "    if result == '': # if the result is empty, display a message\n",
    "        result = 'No documents found'\n",
    "    result_text.delete(1.0, tk.END)  # clear previous results\n",
    "    result_text.insert(tk.END, result) # insert the result into the scrolled text widget\n",
    " \n",
    "root = tk.Tk() # create the main window\n",
    "root.title(\"Information Retrieval System\")\n",
    "\n",
    "query_label = tk.Label(root, text=\"Enter your query:\") # create and place the label widget\n",
    "query_label.pack(pady=5)\n",
    "\n",
    "query_entry = tk.Entry(root, width=50) # create and place the entry widget with width set to 50\n",
    "query_entry.pack(pady=5)\n",
    "\n",
    "search_button = tk.Button(root, text=\"Search\", command=search) # create and place the button widget. It will trigger the search function when clicked\n",
    "search_button.pack(pady=5)\n",
    "\n",
    "result_text = scrolledtext.ScrolledText(root, width=60, height=10) # create and place the scrolled text widget. This will allow the user to scroll through the results\n",
    "result_text.pack(pady=5)\n",
    "\n",
    "root.mainloop() # run the Tkinter event loop\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
