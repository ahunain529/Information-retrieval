{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math as m\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "import customtkinter as ctk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docIDs():\n",
    "    curr_dir = os.getcwd() # get the current directory\n",
    "    docID = [] # create a list to store the document IDs\n",
    "    for i in os.listdir(curr_dir + '\\ResearchPapers'): # loop through each file in the 'ResearchPapers' directory\n",
    "        i = i.rstrip('.txt')\n",
    "        docID.append(int(i))\n",
    "    docID.sort()\n",
    "    return docID\n",
    "\n",
    "def load_stopwords():\n",
    "    stopwords = []\n",
    "    f = open('Stopword-List.txt', 'r') # open the 'Stopword-List.txt' file\n",
    "    while True:\n",
    "        line = f.readline() # each line from the file is read one by one\n",
    "        if not line: # if the line read is empty (which means end of file), the loop is broken\n",
    "            break\n",
    "        stopwords.append(line) # else append the read line to the stopwords list\n",
    "\n",
    "    f.close() # close the file\n",
    "\n",
    "    for i in range(len(stopwords)):\n",
    "        if i != '\\n' and i != '':\n",
    "            stopwords[i] = stopwords[i].rstrip(' \\n') # remove newline characters from the strings\n",
    "        else:\n",
    "            stopwords.pop(i) # remove any empty strings and newline characters from the stopwords list\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def calc_TF(total_tokens):\n",
    "    tf = {} # declare an empty dictionary for the term frequency weights\n",
    "    porter_stemmer = PorterStemmer() # initialize the stemmer\n",
    "    doc = load_docIDs() # get the docIDs\n",
    "\n",
    "    for i, tokens in enumerate(total_tokens): # loop through each token in total_tokens, and then loop through each word in the token\n",
    "        for word in tokens:\n",
    "            word = porter_stemmer.stem(word) # stem the word\n",
    "            if word[-1] == \"'\": # if the word ends with an apostrophe, remove it\n",
    "                word = word.rstrip(\"'\")\n",
    "            if word in tf: # if the word is already in the index, add the docID to the index\n",
    "                if doc[i] in tf[word]:\n",
    "                    freq = tf[word][doc[i]]\n",
    "                    tf[word][doc[i]] = freq + 1\n",
    "                else:\n",
    "                    tf[word][doc[i]] = 1\n",
    "            else: # add the word in the index along with the docID and the frequency\n",
    "                tf[word] = {doc[i]: 1}\n",
    "\n",
    "    for word in tf.keys(): # calculate the log term frequency weights\n",
    "        for doc in tf[word].keys():\n",
    "            tf[word][doc] = 1 + m.log(tf[word][doc], 10) # normalize the term frequency weights\n",
    "\n",
    "    tf = pd.DataFrame(tf) # convert the dictionary to a Pandas DataFrame\n",
    "    tf = tf.transpose() # since the DataFrame will be in the form of words as columns and docIDs as rows, we transpose it to have docIDs as columns and words as rows\n",
    "    tf.fillna(0, inplace=True) # fill the NaN values with 0\n",
    "\n",
    "    print(\"Term Frequency Weights created\")\n",
    "    return tf\n",
    "\n",
    "\n",
    "def calc_IDF(tf):\n",
    "    df = {} # a dictionary to store the document frequency of each word\n",
    "    idf = {} # a dictionary to store the inverse document frequency of each word\n",
    "    doc = load_docIDs() # get the docIDs\n",
    "\n",
    "    for keys in tf.index: # loop through each word in the index\n",
    "        frequency = list(tf.loc[keys].value_counts()) # get the frequency of each word in the document\n",
    "        df[keys] = len(doc) - frequency[0] # calculate the document frequency of each word\n",
    "\n",
    "    for keys in tf.index: # loop through each word in the document frequency index\n",
    "        idf[keys] = m.log(len(doc)/df[keys], 10) # calculate the inverse document frequency of each word\n",
    "\n",
    "    idf = pd.DataFrame(idf, index=[0]) # convert the dictionary to a Pandas DataFrame\n",
    "\n",
    "    print(\"Inverse Document Frequency Weights calculated\")\n",
    "    return idf\n",
    "\n",
    "\n",
    "def preprocessing():\n",
    "    total_tokens = [] # an empty list to store the tokens from all the files\n",
    "    doc = load_docIDs() # get the docIDs\n",
    "    stopwords = load_stopwords() # get the stopwords\n",
    "    stemmer = PorterStemmer() # create a stemmer object\n",
    "\n",
    "    for i in doc: # iterate through each doc\n",
    "        tokens = []\n",
    "        with open('ResearchPapers/' + str(i) + '.txt', 'r') as f: # open the file corresponding to the current document ID\n",
    "            while True:\n",
    "                text = f.readline() # read a line from the file\n",
    "                if not text: # if the line is empty (which means end of file), break the loop\n",
    "                    break\n",
    "                tokens += word_tokenize(text) # tokenize the line and add the tokens to the list\n",
    "\n",
    "        j = 0\n",
    "        while j < len(tokens): # loop through each token\n",
    "            if tokens[j] not in stopwords and len(tokens[j]) <= 45: # filter out the stopwords and tokens with length greater than 45\n",
    "                # remove symbols and numbers from the start and end of the token and also apply case folding\n",
    "                tokens[j] = tokens[j].strip('0123456789!@#$%^&*()-_=+[{]}\\|;:\\'\",<.>/?`~').casefold()\n",
    "                if '.' in tokens[j]: # if '.' exists in a word, split the word at that point and add the splitted words at the end of the tokens list while removing the original word\n",
    "                    word = tokens[j].split('.')\n",
    "                    del tokens[j]\n",
    "                    tokens.extend(word)\n",
    "                elif '-' in tokens[j]: # do the same for words with '-'\n",
    "                    word = tokens[j].split('-')\n",
    "                    del tokens[j]\n",
    "                    tokens.extend(word)\n",
    "            j += 1 # move the index forward\n",
    "        tokens = [stemmer.stem(c) for c in tokens if c.isalpha() and c not in stopwords and len(c) >= 2] # filter out any strings that contain symbols, numbers, etc.\n",
    "        total_tokens.append(tokens) # add the processed tokens as a seperate list. Did this to keep track of which tokens appear in which docs (needed to construct indexes). List at index 0 indicate tokens found in doc 1 and so on.\n",
    "\n",
    "    return total_tokens\n",
    "\n",
    "\n",
    "def calc_TFIDF(TF, IDF):\n",
    "    vector_space = pd.DataFrame(index=TF.index, columns=TF.columns) # create a DataFrame with the same index and columns as the TF DataFrame\n",
    "    for term in TF.index: # loop through each term in the index\n",
    "        if pd.isna(term): # special case of 'nan' term\n",
    "            vector_space.loc[term] = TF.loc[term] * IDF.loc[0, 'nan']\n",
    "        else: # calculate the TF-IDF weights\n",
    "            vector_space.loc[term] = TF.loc[term] * IDF.loc[0, term]\n",
    "    vector_space.transpose() # transpose the DataFrame\n",
    "\n",
    "    print(\"TF-IDF Weights calculated\")\n",
    "    return vector_space\n",
    "\n",
    "\n",
    "def save_weights():\n",
    "    tokens = preprocessing() # preprocessing function is called, returns the processed tokens\n",
    "    tf = calc_TF(tokens) # calculate_TF function is called, returns the TF weights\n",
    "    idf = calc_IDF(tf) # create_positional_index function is called, returns the IDF weights\n",
    "    tf_idf = calc_TFIDF(tf, idf) # calculate the TF-IDF weights\n",
    "\n",
    "    tf_idf.to_csv('tf-idf.csv') # output TF-IDF DataFrame to CSV including the index\n",
    "    print(\"TF-IDF Weights saved\")\n",
    "\n",
    "    idf.to_csv('idf.csv') # output IDF DataFrame to CSV including the index\n",
    "    print(\"Inverse Document Frequency Weights saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are already calculated\n"
     ]
    }
   ],
   "source": [
    "if (not os.path.isfile('tf-idf.csv') or not os.path.isfile('idf.csv')): # check if the indexes already exist, if they don't, call the save_indexes function\n",
    "    save_weights()\n",
    "else:\n",
    "    print(\"Weights are already calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weights():\n",
    "    tf_idf = pd.read_csv('tf-idf.csv', index_col=0)  # read TF-IDF DataFrame from CSV\n",
    "    idf = pd.read_csv('idf.csv', index_col=0) # read IDF DataFrame from CSV\n",
    "\n",
    "    return tf_idf, idf\n",
    "\n",
    "def calc_queryvector(query, IDF):\n",
    "    query_vector = {} # create a list of zeros with the length of the columns in the IDF DataFrame\n",
    "\n",
    "    for term in IDF.columns: # loop through each term in the IDF columns\n",
    "        if term in query: # if the term is in the query\n",
    "            query_vector[term] = 1 + m.log(query.count(term), 10) # calculate the log term frequency weight\n",
    "            query_vector[term] *= IDF.loc[0, term] # multiply the log term frequency weight by the IDF weight\n",
    "        else:\n",
    "            query_vector[term] = 0 # if the term is not in the query, set the weight to zero\n",
    "\n",
    "    norm = sum([query_vector[x] ** 2 for x in query_vector.keys()]) ** 0.5 # calculate the norm of the query vector\n",
    "\n",
    "    if norm != 0: # if the norm is not zero\n",
    "        for term in query_vector:\n",
    "            query_vector[term] = query_vector[term] / norm # normalize the query vector\n",
    "\n",
    "    return query_vector\n",
    "\n",
    "def query_processing(query):\n",
    "    vectors, IDF = extract_weights() # # read the TF-IDF and IDF DataFrames from CSV\n",
    "\n",
    "    query_vector = calc_queryvector(query, IDF) # calculate the query vector\n",
    "    query_vector = pd.DataFrame(query_vector, index=[0]) # create a DataFrame from the query vector\n",
    "\n",
    "    return vectors, query_vector\n",
    "\n",
    "def calc_sim(query):\n",
    "    query = query.split() # split the query into words\n",
    "    porter_stemmer = PorterStemmer() # initialize the stemmer\n",
    "    stopwords = load_stopwords() # get the stopwords\n",
    "    doc = load_docIDs() # get the document IDs\n",
    "\n",
    "    query = [porter_stemmer.stem(word).rstrip(\"'\").casefold() for word in query if word not in stopwords] # stem the words in the query and remove the stopwords\n",
    "    \n",
    "    vectors , query_vector = query_processing(query) # calculate the query vector\n",
    "\n",
    "    score = {} # create a dictionary to store the similarity scores\n",
    "    for docID in doc:\n",
    "        score[docID] = np.dot(vectors[str(docID)], query_vector.transpose()) # calculate the similarity score for each document\n",
    "\n",
    "    score = {k: v for k, v in sorted(score.items(), key=lambda item: item[1], reverse=True)} # sort the similarity scores in descending order\n",
    "    score = {k: score[k] for k in score if score[k] >= 0.05} # remove any documents with a similarity score less than 0.05\n",
    "\n",
    "    score = [k for k in score.keys()]\n",
    "    score = ' '.join(map(str, score))\n",
    "\n",
    "    return score\n",
    "\n",
    "def process_query():\n",
    "    query = entry.get() # get the query from the text entry field\n",
    "    result = calc_sim(query)\n",
    "\n",
    "    if result == '': # if no documents are found\n",
    "        result = 'No documents found'\n",
    "\n",
    "    output_label.configure(state='normal') # enable the output label\n",
    "    output_label.delete(0, tk.END) # clear the output label\n",
    "    output_label.insert(0, result) # insert the result into the output label\n",
    "    output_label.configure(state='readonly') # again disable the output label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctk.set_appearance_mode('Dark') # set the appearance mode to dark\n",
    "ctk.set_default_color_theme('dark-blue') # set the default color theme\n",
    "\n",
    "root = ctk.CTk() # create a new window\n",
    "root.geometry('500x400') # set the window size\n",
    "root.title('Vector Space Retrieval Model') # set the window title\n",
    "\n",
    "# create a label \"Vector Space Retrieval Model\" with a font size of 20 and transparent foreground color\n",
    "label1 = ctk.CTkLabel(\n",
    "    root,\n",
    "    text=\"Vector Space Retrieval Model\",\n",
    "    font=(\"Verdana\", 20),\n",
    "    fg_color=\"transparent\"\n",
    ")\n",
    "# place the label according to the given co-ordinated relative to x and y axis\n",
    "label1.place(\n",
    "    relx=0.5,\n",
    "    rely=0.2,\n",
    "    anchor=tk.CENTER\n",
    ")\n",
    "\n",
    "# create another label \"Enter Query\" with a transparent foreground color\n",
    "label2 = ctk.CTkLabel(\n",
    "    root,\n",
    "    text=\"Enter Query\",\n",
    "    fg_color=\"transparent\"\n",
    ")\n",
    "# place the at the center of the window\n",
    "label2.place( \n",
    "    relx=0.5,\n",
    "    rely=0.3,\n",
    "    anchor=tk.CENTER\n",
    ")\n",
    "\n",
    "# create a text entry field with a width of 200 and a black background color\n",
    "entry = ctk.CTkEntry(\n",
    "    root,\n",
    "    width=200,\n",
    "    bg_color='black'\n",
    ")\n",
    "# place the text entry field in the window\n",
    "entry.place(\n",
    "    relx=0.5,\n",
    "    rely=0.4,\n",
    "    anchor=tk.CENTER\n",
    ")\n",
    "\n",
    "# create a button \"Process Query\" with a font size of 12 and white background color and black text color. The button calls the process_query function when clicked\n",
    "process_button = ctk.CTkButton(\n",
    "    root,\n",
    "    text=\"Process Query\",\n",
    "    font=(\"Helvetica\", 12),\n",
    "    bg_color='white',\n",
    "    fg_color=\"#B6C8A9\",\n",
    "    hover_color=\"white\",\n",
    "    text_color = \"black\",\n",
    "    command=process_query\n",
    ")\n",
    "# place the button in the window\n",
    "process_button.place(\n",
    "    relx=0.5,\n",
    "    rely=0.5,\n",
    "    anchor=tk.CENTER\n",
    ")\n",
    "\n",
    "# create a button \"Exit\" with a font size of 12 with white background color and black text color. The button terminates the window when clicked\n",
    "exit_button = ctk.CTkButton(\n",
    "    root,\n",
    "    text=\"Exit\",\n",
    "    font=(\"Helvetica\", 12),\n",
    "    bg_color='white',\n",
    "    fg_color=\"#B6C8A9\",\n",
    "    hover_color=\"white\",\n",
    "    text_color = \"black\",\n",
    "    command=root.destroy\n",
    ")\n",
    "# place the button in the window\n",
    "exit_button.place(\n",
    "    relx=0.5,\n",
    "    rely=0.6,\n",
    "    anchor=tk.CENTER\n",
    ")\n",
    "\n",
    "# create a text entry field with a width of 400, a height of 50, and a black background color\n",
    "output_label = ctk.CTkEntry(\n",
    "    root,\n",
    "    width=400,\n",
    "    height=50,\n",
    "    bg_color='black'\n",
    ")\n",
    "# place the text entry field in the window\n",
    "output_label.place(\n",
    "    relx=0.5,\n",
    "    rely=0.8,\n",
    "    anchor=tk.CENTER\n",
    ")\n",
    "# set the state of the text entry field to readonly (disable it)\n",
    "output_label.configure(\n",
    "    state='readonly'\n",
    ")\n",
    "\n",
    "root.mainloop() # run the window"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
