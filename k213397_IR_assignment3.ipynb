{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math as m\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docIDs():\n",
    "    curr_dir = os.getcwd() # get the current directory\n",
    "    docID = [] # create a list to store the document IDs\n",
    "    for i in os.listdir(curr_dir + '\\ResearchPapers'): # loop through each file in the 'ResearchPapers' directory\n",
    "        i = i.rstrip('.txt')\n",
    "        docID.append(int(i))\n",
    "    docID.sort()\n",
    "    return docID\n",
    "\n",
    "def load_stopwords():\n",
    "    stopwords = []\n",
    "    f = open('Stopword-List.txt', 'r') # open the 'Stopword-List.txt' file\n",
    "    while True:\n",
    "        line = f.readline() # each line from the file is read one by one\n",
    "        if not line: # if the line read is empty (which means end of file), the loop is broken\n",
    "            break\n",
    "        stopwords.append(line) # else append the read line to the stopwords list\n",
    "\n",
    "    f.close() # close the file\n",
    "\n",
    "    for i in range(len(stopwords)):\n",
    "        if i != '\\n' and i != '':\n",
    "            stopwords[i] = stopwords[i].rstrip(' \\n') # remove newline characters from the strings\n",
    "        else:\n",
    "            stopwords.pop(i) # remove any empty strings and newline characters from the stopwords list\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def calc_TF(total_tokens):\n",
    "    tf = {} # declare an empty dictionary for the term frequency weights\n",
    "    porter_stemmer = PorterStemmer() # initialize the stemmer\n",
    "    doc = load_docIDs() # get the docIDs\n",
    "\n",
    "    for i, tokens in enumerate(total_tokens): # loop through each token in total_tokens, and then loop through each word in the token\n",
    "        for word in tokens:\n",
    "            word = porter_stemmer.stem(word) # stem the word\n",
    "            if word[-1] == \"'\": # if the word ends with an apostrophe, remove it\n",
    "                word = word.rstrip(\"'\")\n",
    "            if word in tf: # if the word is already in the index, add the docID to the index\n",
    "                if doc[i] in tf[word]:\n",
    "                    freq = tf[word][doc[i]]\n",
    "                    tf[word][doc[i]] = freq + 1\n",
    "                else:\n",
    "                    tf[word][doc[i]] = 1\n",
    "            else: # add the word in the index along with the docID and the frequency\n",
    "                tf[word] = {doc[i]: 1}\n",
    "\n",
    "    for word in tf.keys(): # calculate the log term frequency weights\n",
    "        for doc in tf[word].keys():\n",
    "            tf[word][doc] = 1 + m.log(tf[word][doc], 10) # normalize the term frequency weights\n",
    "\n",
    "    tf = pd.DataFrame(tf) # convert the dictionary to a Pandas DataFrame\n",
    "    tf = tf.transpose() # since the DataFrame will be in the form of words as columns and docIDs as rows, we transpose it to have docIDs as columns and words as rows\n",
    "    tf.fillna(0, inplace=True) # fill the NaN values with 0\n",
    "\n",
    "    print(\"Term Frequency Weights created\")\n",
    "    return tf\n",
    "\n",
    "\n",
    "def calc_IDF(tf):\n",
    "    df = {} # a dictionary to store the document frequency of each word\n",
    "    idf = {} # a dictionary to store the inverse document frequency of each word\n",
    "    doc = load_docIDs() # get the docIDs\n",
    "\n",
    "    for keys in tf.index: # loop through each word in the index\n",
    "        frequency = list(tf.loc[keys].value_counts()) # get the frequency of each word in the document\n",
    "        df[keys] = len(doc) - frequency[0] # calculate the document frequency of each word\n",
    "\n",
    "    for keys in tf.index: # loop through each word in the document frequency index\n",
    "        idf[keys] = m.log(len(doc)/df[keys], 10) # calculate the inverse document frequency of each word\n",
    "\n",
    "    idf = pd.DataFrame(idf, index=[0]) # convert the dictionary to a Pandas DataFrame\n",
    "\n",
    "    print(\"Inverse Document Frequency Weights calculated\")\n",
    "    return idf\n",
    "\n",
    "\n",
    "def preprocessing():\n",
    "    total_tokens = [] # an empty list to store the tokens from all the files\n",
    "    doc = load_docIDs() # get the docIDs\n",
    "    stopwords = load_stopwords() # get the stopwords\n",
    "    stemmer = PorterStemmer() # create a stemmer object\n",
    "\n",
    "    for i in doc: # iterate through each doc\n",
    "        tokens = []\n",
    "        with open('ResearchPapers/' + str(i) + '.txt', 'r') as f: # open the file corresponding to the current document ID\n",
    "            while True:\n",
    "                text = f.readline() # read a line from the file\n",
    "                if not text: # if the line is empty (which means end of file), break the loop\n",
    "                    break\n",
    "                tokens += word_tokenize(text) # tokenize the line and add the tokens to the list\n",
    "\n",
    "        j = 0\n",
    "        while j < len(tokens): # loop through each token\n",
    "            if tokens[j] not in stopwords and len(tokens[j]) <= 45: # filter out the stopwords and tokens with length greater than 45\n",
    "                # remove symbols and numbers from the start and end of the token and also apply case folding\n",
    "                tokens[j] = tokens[j].strip('0123456789!@#$%^&*()-_=+[{]}\\|;:\\'\",<.>/?`~').casefold()\n",
    "                if '.' in tokens[j]: # if '.' exists in a word, split the word at that point and add the splitted words at the end of the tokens list while removing the original word\n",
    "                    word = tokens[j].split('.')\n",
    "                    del tokens[j]\n",
    "                    tokens.extend(word)\n",
    "                elif '-' in tokens[j]: # do the same for words with '-'\n",
    "                    word = tokens[j].split('-')\n",
    "                    del tokens[j]\n",
    "                    tokens.extend(word)\n",
    "            j += 1 # move the index forward\n",
    "        tokens = [stemmer.stem(c) for c in tokens if c.isalpha() and c not in stopwords and len(c) >= 2] # filter out any strings that contain symbols, numbers, etc.\n",
    "        total_tokens.append(tokens) # add the processed tokens as a seperate list. Did this to keep track of which tokens appear in which docs (needed to construct indexes). List at index 0 indicate tokens found in doc 1 and so on.\n",
    "\n",
    "    return total_tokens\n",
    "\n",
    "\n",
    "def calc_TFIDF(TF, IDF):\n",
    "    vector_space = pd.DataFrame(index=TF.index, columns=TF.columns) # create a DataFrame with the same index and columns as the TF DataFrame\n",
    "    for term in TF.index: # loop through each term in the index\n",
    "        if pd.isna(term): # special case of 'nan' term\n",
    "            vector_space.loc[term] = TF.loc[term] * IDF.loc[0, 'nan']\n",
    "        else: # calculate the TF-IDF weights\n",
    "            vector_space.loc[term] = TF.loc[term] * IDF.loc[0, term]\n",
    "    \n",
    "    vector_space = vector_space.transpose()\n",
    "\n",
    "    print(\"TF-IDF Weights calculated\")\n",
    "    return vector_space\n",
    "\n",
    "\n",
    "def save_weights():\n",
    "    tokens = preprocessing() # preprocessing function is called, returns the processed tokens\n",
    "    tf = calc_TF(tokens) # calculate_TF function is called, returns the TF weights\n",
    "    idf = calc_IDF(tf) # create_positional_index function is called, returns the IDF weights\n",
    "    tf_idf = calc_TFIDF(tf, idf) # calculate the TF-IDF weights\n",
    "\n",
    "    tf_idf.to_csv('tf-idf.csv') # output TF-IDF DataFrame to CSV including the index\n",
    "    print(\"TF-IDF Weights saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency Weights created\n",
      "Inverse Document Frequency Weights calculated\n",
      "TF-IDF Weights calculated\n",
      "TF-IDF Weights saved\n"
     ]
    }
   ],
   "source": [
    "if (not os.path.isfile('tf-idf.csv')): # check if the indexes already exist, if they don't, call the save_indexes function\n",
    "    save_weights()\n",
    "else:\n",
    "    print(\"Weights are already calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, silhouette_score, adjusted_rand_score\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weights():\n",
    "    TF_IDF = pd.read_csv('tf-idf.csv', index_col=0)  # read TF-IDF DataFrame from CSV\n",
    "    return TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(tf_idf):\n",
    "    label1 = ['1', '2', '3', '7'] # label for \"Explainable Artificial Intelligence\"\n",
    "    label2 = ['8', '9', '11'] # label for \"Heart Failure\"\n",
    "    label3 = ['12', '13', '14', '15', '16'] # label for \"Time Series Forecasting\"\n",
    "    label4 = ['17', '18', '21'] # label for \"Transformer Model\"\n",
    "    label5 = ['22', '23', '24', '25', '26'] # label for \"Feature Selection\"\n",
    "\n",
    "    for index in tf_idf.index:\n",
    "        if index in label1:\n",
    "            tf_idf['label'] = 1\n",
    "        elif index in label2:\n",
    "            tf_idf['label'] = 2\n",
    "        elif index in label3:\n",
    "            tf_idf['label'] = 3\n",
    "        elif index in label4:\n",
    "            tf_idf['label'] = 4\n",
    "        elif index in label5:\n",
    "            tf_idf['label'] = 5\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(tf_idf):\n",
    "    # split the data into training and testing sets\n",
    "    y = tf_idf['label'].apply(int) # convert the labels to integers\n",
    "    X = tf_idf.drop('label', axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 80% training and 20% testing\n",
    "\n",
    "    # train the model\n",
    "    model = KNeighborsClassifier(n_neighbors=5) # create a KNN model\n",
    "    model.fit(X_train, y_train) # train the model\n",
    "\n",
    "    return model, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metric(model, X_test, y_test):\n",
    "    # test the model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    metric = {}\n",
    "\n",
    "    # evaluate the model\n",
    "    metric['accuracy'] = accuracy_score(y_test, y_pred)\n",
    "    metric['recall'] = recall_score(y_test, y_pred, average='weighted')\n",
    "    metric['f1'] = f1_score(y_test, y_pred, average='weighted')\n",
    "    metric['precision'] = precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster(df, max_k):\n",
    "    # using elbow method for finding the optimal number of clusters\n",
    "\n",
    "    sse = [] # a list to store sum of squared errors\n",
    "    for k in range(2, max_k+1, 2): # loop through the range of clusters\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(df) # fit the model\n",
    "        sse.append(kmeans.inertia_)\n",
    "\n",
    "    plt.plot(range(2, max_k+1, 2), sse, marker='o') # plot the number of clusters against the sum of squared errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(df):\n",
    "    kmeans = KMeans(n_clusters=19, random_state=42) # create a KMeans model with 19 clusters\n",
    "    y_predict = kmeans.fit_predict(df) # fit the model and predict the clusters\n",
    "    df['cluster'] = y_predict # add the predicted clusters to the DataFrame\n",
    "    return kmeans, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    contingency = contingency_matrix(y_true, y_pred) # Compute contingency matrix\n",
    "    cluster_purities = np.sum(np.max(contingency, axis=0)) # Sum of the maximum values in each cluster (cluster purity)\n",
    "    total_samples = np.sum(contingency) # Total number of samples\n",
    "    purity = cluster_purities / total_samples # Compute purity score\n",
    "    return purity\n",
    "\n",
    "\n",
    "def clust_metrics(df, y_predict):\n",
    "    metrics = {} # a dictionary to store the evaluation metrics\n",
    "\n",
    "    df = add_labels(df)\n",
    "\n",
    "    # print evaluation metrics\n",
    "    metrics[\"Purity\"] = purity_score(df['label'], y_predict)\n",
    "    metrics[\"Silhouette Score\"] = silhouette_score(df, y_predict)\n",
    "    metrics[\"Random Index\"] = adjusted_rand_score(df['label'], y_predict)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, Canvas # for combobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_train_and_eval():\n",
    "    tf_idf = extract_weights()\n",
    "    tf_idf = add_labels(tf_idf)\n",
    "    model, X_test, y_test = training_model(tf_idf)\n",
    "    metrics = evaluation_metric(model, X_test, y_test)\n",
    "    return metrics\n",
    "    # Display classification metrics\n",
    "\n",
    "# Function to perform clustering\n",
    "def clustering_train_and_eval():\n",
    "    tf_idf = extract_weights()\n",
    "    model, df = model_train(tf_idf)\n",
    "    metrics = clust_metrics(df, df['cluster'])\n",
    "    return metrics\n",
    "\n",
    "def perform_classification():\n",
    "    close_window()\n",
    "    display_metrics(\"Classification\")\n",
    "\n",
    "def perform_clustering():\n",
    "    close_window()\n",
    "    display_metrics(\"Clustering\")\n",
    "\n",
    "def close_window():\n",
    "    root.withdraw()\n",
    "\n",
    "def display_metrics(algorithm):\n",
    "    close_window()\n",
    "\n",
    "    if algorithm == \"Classification\":\n",
    "        metrics = classification_train_and_eval()\n",
    "    elif algorithm == \"Clustering\":\n",
    "        metrics = clustering_train_and_eval()\n",
    "\n",
    "    new_window = tk.Tk()\n",
    "    new_window.title(f\"{algorithm} Metrics\")\n",
    "\n",
    "    # Display metrics in the new window\n",
    "    row = 0\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        ttk.Label(new_window, text=f\"{metric_name}: {metric_value}\").grid(row=row, column=0, padx=5, pady=2, sticky=\"w\")\n",
    "        row += 1\n",
    "\n",
    "    # Add an exit button\n",
    "    ttk.Button(new_window, text=\"Exit\", command=new_window.destroy).grid(row=row, column=0, padx=5, pady=5)\n",
    "    root.destroy()\n",
    "\n",
    "    new_window.mainloop()\n",
    "\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Text Classification and Clustering\")\n",
    "\n",
    "algorithm_frame = ttk.Frame(root)\n",
    "algorithm_frame.pack(padx=10, pady=10)\n",
    "\n",
    "algorithm_label = ttk.Label(algorithm_frame, text=\"Select Algorithm:\")\n",
    "algorithm_label.grid(row=0, column=0, padx=5, pady=5, sticky=\"w\")\n",
    "algorithm_selection = ttk.Combobox(algorithm_frame, values=[\"Classification\", \"Clustering\"])\n",
    "algorithm_selection.grid(row=0, column=1, padx=5, pady=5)\n",
    "algorithm_selection.set(\"Classification\")  # Set default selection\n",
    "\n",
    "perform_button = ttk.Button(algorithm_frame, text=\"Perform\", command=lambda: perform_task(algorithm_selection.get()))\n",
    "perform_button.grid(row=1, column=1, padx=5, pady=5)\n",
    "\n",
    "def perform_task(selected_algorithm):\n",
    "    if selected_algorithm == \"Classification\":\n",
    "        perform_classification()\n",
    "    elif selected_algorithm == \"Clustering\":\n",
    "        perform_clustering()\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
